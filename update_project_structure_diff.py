#!/usr/bin/env python3
"""
update_project_structure_diff.py
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
‚Ä¢ Qu√©t c·∫•u tr√∫c d·ª± √°n, sinh c√¢y Markdown + JSON.
‚Ä¢ So s√°nh v·ªõi snapshot c≈© ‚Üí t·∫°o diff Markdown.
‚Ä¢ T·ª± ƒë·ªông sinh JSON Schema cho *.json* / *.jsonl* (m·ªõi ho·∫∑c ch·ªânh s·ª≠a).
"""

from __future__ import annotations
import hashlib
import json
import os
import sys
from datetime import datetime
from typing import Dict, Set, Tuple

# ======= C·∫§U H√åNH ======= #
PROJECT_ROOT  = r"F:\dhdv_stack_v_2.9"

SNAPSHOT_JSON = os.path.join(PROJECT_ROOT, ".project_snapshot.json")
OUTPUT_JSON   = os.path.join(PROJECT_ROOT, "project_structure.json")
OUTPUT_MD     = os.path.join(PROJECT_ROOT, "project_structure.md")
OUTPUT_DIFF   = os.path.join(PROJECT_ROOT, "project_structure_diff.md")

SCHEMAS_DIR   = os.path.join(PROJECT_ROOT, "schemas")    # n∆°i l∆∞u *.schema.json
MAX_SAMPLE_LINES = 500                                   # l·∫•y m·∫´u t·ªëi ƒëa ‚Ä¶ d√≤ng khi sinh schema

EXCLUDE_DIRS  = { "node_modules", ".git", "venv", ".venv",
                  "__pycache__", ".pytest_cache", ".mypy_cache",
                  "schemas" }        # tr√°nh v√≤ng l·∫∑p khi scan
HASH_EXT      = { ".py", ".ipynb", ".md", ".json", ".yaml", ".yml", ".txt" }
LOG_STEP      = 1_000               # in log m·ªói ‚Ä¶ file
MAX_HASH_SIZE = 20 * 1024**2        # (byte) >20‚ÄØMB ‚Üí kh√¥ng bƒÉm
# ========================= #


# --------- TI·ªÜN √çCH --------- #
def file_sha1(path: str, blk: int = 2**16) -> str:
    sha1 = hashlib.sha1()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(blk), b""):
            sha1.update(chunk)
    return sha1.hexdigest()


def build_snapshot(root: str, verbose: bool = True) -> Dict[str, str | None]:
    """
    Tr·∫£ v·ªÅ dict {rel_path: sha1 | None}.
    ‚Ä¢ B·ªè qua EXCLUDE_DIRS.
    ‚Ä¢ V·ªõi file > MAX_HASH_SIZE ho·∫∑c ƒëu√¥i kh√¥ng thu·ªôc HASH_EXT ‚Üí sha = None.
    """
    snap, cnt = {}, 0
    for cur_root, dirs, files in os.walk(root):
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            if fname == os.path.basename(__file__):
                continue
            cnt += 1
            fpath = os.path.join(cur_root, fname)
            rel   = os.path.relpath(fpath, root).replace("\\", "/")
            size  = os.path.getsize(fpath)
            ext   = os.path.splitext(fname)[1].lower()

            if size > MAX_HASH_SIZE or ext not in HASH_EXT:
                snap[rel] = None
            else:
                snap[rel] = file_sha1(fpath)

            if verbose and cnt % LOG_STEP == 0:
                print(f"‚Ä¶ƒë√£ x·ª≠ l√Ω {cnt:,} file")
    if verbose:
        print(f"‚û°Ô∏è  Ho√†n t·∫•t, ƒë√£ qu√©t {cnt:,} file (ƒë√£ exclude)")
    return snap


def snapshot_to_tree(snapshot: Dict[str, str | None]) -> Dict:
    tree: Dict = {}
    for path in snapshot:
        parts, node = path.split("/"), tree
        for part in parts[:-1]:
            node = node.setdefault(part, {})
        node[parts[-1]] = None
    return tree


def tree_to_md(tree: Dict, indent: int = 0) -> list[str]:
    lines: list[str] = []
    for key in sorted(tree):
        lines.append("  " * indent + f"- {key}")
        if isinstance(tree[key], dict):
            lines.extend(tree_to_md(tree[key], indent + 1))
    return lines


def save_json(obj: Dict, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def save_text(lines: list[str], path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


def load_snapshot() -> Dict | None:
    if os.path.exists(SNAPSHOT_JSON):
        with open(SNAPSHOT_JSON, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def diff_snap(prev: Dict, curr: Dict) -> Tuple[Set[str], Set[str], Set[str]]:
    prev_files, curr_files = set(prev), set(curr)
    added    = curr_files - prev_files
    removed  = prev_files - curr_files
    modified = {
        p for p in (curr_files & prev_files)
        if prev.get(p) != curr.get(p)
    }
    return added, removed, modified


def write_diff_md(added: Set[str], removed: Set[str], modified: Set[str]):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    lines = [f"# üìÇ Project structure diff ‚Äî {ts}", ""]

    def sec(title: str, items: Set[str]):
        if items:
            lines.extend([f"## {title} ({len(items)})",
                          *[f"- {p}" for p in sorted(items)],
                          ""])

    sec("‚ûï Th√™m m·ªõi", added)
    sec("‚ûñ ƒê√£ xo√°", removed)
    sec("‚úèÔ∏è Thay ƒë·ªïi n·ªôi dung", modified)
    save_text(lines, OUTPUT_DIFF)


# ---------- JSON SCHEMA ---------- #
try:
    from genson import SchemaBuilder
except ImportError:
    print("‚ö†Ô∏è  Thi·∫øu th∆∞ vi·ªán 'genson'. C√†i b·∫±ng:  pip install genson", file=sys.stderr)
    SchemaBuilder = None   # type: ignore


def build_json_schema(path: str, max_lines: int = MAX_SAMPLE_LINES) -> Dict:
    """
    ƒê·ªçc t·ªëi ƒëa max_lines (ho·∫∑c to√†n b·ªô v·ªõi *.json*) ƒë·ªÉ sinh schema.
    """
    builder = SchemaBuilder()
    ext = os.path.splitext(path)[1].lower()

    if ext == ".jsonl":
        with open(path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                if i >= max_lines:
                    break
                line = line.strip()
                if line:
                    builder.add_object(json.loads(line))
    else:  # .json
        with open(path, "r", encoding="utf-8") as f:
            try:
                obj = json.load(f)
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è  Kh√¥ng parse ƒë∆∞·ª£c JSON: {path}", file=sys.stderr)
                return {}
            builder.add_object(obj)
    return builder.to_schema()


def dump_schema(rel_path: str):
    """
    L∆∞u schema v√†o SCHEMAS_DIR, gi·ªØ nguy√™n c·∫•u tr√∫c th∆∞ m·ª•c.
    """
    if SchemaBuilder is None:
        return  # genson ch∆∞a c√†i
    in_path  = os.path.join(PROJECT_ROOT, rel_path)
    out_path = os.path.join(SCHEMAS_DIR, f"{rel_path}.schema.json")
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    schema = build_json_schema(in_path)
    if schema:
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(schema, f, ensure_ascii=False, indent=2)
        print(f"üìÑ  ƒê√£ ghi schema: {out_path}")
# ---------------------------------- #


def candidate_for_schema(rel_path: str) -> bool:
    ext = os.path.splitext(rel_path)[1].lower()
    return ext in {".json", ".jsonl"}


def main():
    # 1. Qu√©t hi·ªán tr·∫°ng
    curr_snap = build_snapshot(PROJECT_ROOT)

    # 2. Ghi c·∫•u tr√∫c (.json + .md)
    tree = snapshot_to_tree(curr_snap)
    save_json(tree, OUTPUT_JSON)
    save_text(tree_to_md(tree), OUTPUT_MD)

    # 3. So s√°nh v·ªõi snapshot c≈© (n·∫øu c√≥)
    prev_snap = load_snapshot()
    if prev_snap:
        added, removed, modified = diff_snap(prev_snap, curr_snap)
        write_diff_md(added, removed, modified)
        print(f"‚ûï Added   : {len(added)}")
        print(f"‚ûñ Removed : {len(removed)}")
        print(f"‚úèÔ∏è Modified: {len(modified)}")
        changed_for_schema = added | modified
    else:
        print("‚ÑπÔ∏è L·∫ßn qu√©t ƒë·∫ßu ‚Äì t·∫°o snapshot, b·ªè qua diff.")
        changed_for_schema = set(curr_snap)  # sinh schema cho t·∫•t c·∫£ l·∫ßn ƒë·∫ßu

    # 4. Sinh JSON Schema cho file JSON/JSONL m·ªõi ho·∫∑c thay ƒë·ªïi
    json_targets = [p for p in changed_for_schema if candidate_for_schema(p)]
    if json_targets:
        for p in json_targets:
            dump_schema(p)
    else:
        print("‚ÑπÔ∏è  Kh√¥ng c√≥ file JSON/JSONL m·ªõi ho·∫∑c thay ƒë·ªïi ‚Üí b·ªè qua sinh schema.")

    # 5. L∆∞u snapshot cho l·∫ßn sau
    save_json(curr_snap, SNAPSHOT_JSON)
    print("‚úÖ  Ho√†n t·∫•t c·∫≠p nh·∫≠t project structure & diff (n·∫øu c√≥).")


if __name__ == "__main__":
    main()
